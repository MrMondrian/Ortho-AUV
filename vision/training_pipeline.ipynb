{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations\n",
    "!pip install opencv-python\n",
    "!pip install ultralytics\n",
    "!pip install roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov9'...\n",
      "remote: Enumerating objects: 325, done.\u001b[K\n",
      "remote: Counting objects: 100% (202/202), done.\u001b[K\n",
      "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
      "remote: Total 325 (delta 152), reused 118 (delta 118), pack-reused 123\u001b[K\n",
      "Receiving objects: 100% (325/325), 2.26 MiB | 24.00 KiB/s, done.\n",
      "Resolving deltas: 100% (161/161), done.\n",
      "/home/potenza/Ortho-AUV/vision/yolov9\n"
     ]
    }
   ],
   "source": [
    "# If you don't have yolov9 set up\n",
    "!git clone https://github.com/SkalskiP/yolov9.git\n",
    "%cd yolov9\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/potenza/Ortho-AUV/vision/yolov9\n"
     ]
    }
   ],
   "source": [
    "%cd yolov9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Exporting format yolov9 in progress : 85.0%\n",
      "Version export complete for yolov9 format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in COMP400-Fish-detection-2 to yolov9:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:00<00:00, 3032.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to COMP400-Fish-detection-2 in yolov9:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 11098.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key=\"HV9jB1dcJZHZd1YO5S7C\") \n",
    "project = rf.workspace(\"comp400\").project(\"comp400-fish-detection\")\n",
    "dataset = project.version(2).download(\"yolov9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt\n",
    "!wget -P weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt\n",
    "!wget -P weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-c.pt\n",
    "!wget -P weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-e.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=weights/gelan-c.pt, cfg=models/detect/gelan-c.yaml, data=COMP400-Fish-detection-2/data.yaml, hyp=hyp.scratch-high.yaml, epochs=20, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=cpu, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=15, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "YOLOv5 ðŸš€ 1e33dbb Python-3.8.10 torch-2.2.1+cu121 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, dfl=1.5, obj_pw=1.0, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO ðŸš€ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO ðŸš€ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n",
      "  3                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      "  4                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n",
      "  5                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  6                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
      "  7                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  8                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
      "  9                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]               \n",
      " 10                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 11           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 12                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]      \n",
      " 16                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 17          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 18                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]       \n",
      " 19                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 20           [-1, 9]  1         0  models.common.Concat                    [1]                           \n",
      " 21                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
      " 22      [15, 18, 21]  1   5491411  models.yolo.DDetect                     [1, [256, 512, 512]]          \n",
      "gelan-c summary: 621 layers, 25437843 parameters, 25437827 gradients, 103.2 GFLOPs\n",
      "\n",
      "Transferred 931/937 items from weights/gelan-c.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 154 weight(decay=0.0), 161 weight(decay=0.0005), 160 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/potenza/Ortho-AUV/vision/yolov9/COMP400-Fish-detection-2/t\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/potenza/Ortho-AUV/vision/yolov9/COMP400-Fish-detection-2/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/potenza/Ortho-AUV/vision/yolov9/COMP400-Fish-detection-2/val\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/potenza/Ortho-AUV/vision/yolov9/COMP400-Fish-detection-2/valid/labels.cache\n",
      "Plotting labels to runs/train/exp3/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 3 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp3\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       0/19         0G      2.844      5.669      2.765          2        640:  Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/potenza/Ortho-AUV/vision/yolov9/utils/plots.py\", line 300, in plot_images\n",
      "    annotator.box_label(box, label, color=color)\n",
      "  File \"/home/potenza/Ortho-AUV/vision/yolov9/utils/plots.py\", line 86, in box_label\n",
      "    w, h = self.font.getsize(label)  # text width, height\n",
      "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
      "       0/19         0G      2.844      5.669      2.765          2        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          1          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/19         0G      2.396       6.58      2.229          2        640: 1Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/potenza/Ortho-AUV/vision/yolov9/utils/plots.py\", line 300, in plot_images\n",
      "    annotator.box_label(box, label, color=color)\n",
      "  File \"/home/potenza/Ortho-AUV/vision/yolov9/utils/plots.py\", line 86, in box_label\n",
      "    w, h = self.font.getsize(label)  # text width, height\n",
      "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
      "       1/19         0G      2.396       6.58      2.229          2        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          1          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/19         0G          0      21.42          0          0        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          1          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/19         0G      2.063      9.477     0.9336          1        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          1          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/19         0G      2.189      6.296      2.161          3        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          1          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/19         0G          0      17.83          0          0        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          1          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/19         0G       2.41      7.608      2.055          1        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          1          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/19         0G      2.442      8.207      2.483          1        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          1          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/1 00:00^C\n",
      "  0%|          | 0/1 00:01\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 634, in <module>\n",
      "    main(opt)\n",
      "  File \"train.py\", line 528, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"train.py\", line 303, in train\n",
      "    pred = model(imgs)  # forward\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/potenza/Ortho-AUV/vision/yolov9/models/yolo.py\", line 579, in forward\n",
      "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n",
      "  File \"/home/potenza/Ortho-AUV/vision/yolov9/models/yolo.py\", line 481, in _forward_once\n",
      "    x = m(x)  # run\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/potenza/Ortho-AUV/vision/yolov9/models/common.py\", line 592, in forward\n",
      "    y.extend((m(y[-1])) for m in [self.cv2, self.cv3])\n",
      "  File \"/home/potenza/Ortho-AUV/vision/yolov9/models/common.py\", line 592, in <genexpr>\n",
      "    y.extend((m(y[-1])) for m in [self.cv2, self.cv3])\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/potenza/Ortho-AUV/vision/yolov9/models/common.py\", line 384, in forward\n",
      "    return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/potenza/Ortho-AUV/vision/yolov9/models/common.py\", line 309, in forward\n",
      "    return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/potenza/Ortho-AUV/vision/yolov9/models/common.py\", line 111, in forward\n",
      "    return self.act(self.conv1(x) + self.conv2(x) + id_out)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/potenza/Ortho-AUV/vision/yolov9/models/common.py\", line 54, in forward\n",
      "    return self.act(self.bn(self.conv(x)))\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 460, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/potenza/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py \\\n",
    "     --batch 16 --epochs 20 --img 640 --device 0 --min-items 0 --close-mosaic 15 \\\n",
    "     --data COMP400-Fish-detection-2/data.yaml \\\n",
    "     --weights weights/gelan-c.pt \\\n",
    "     --cfg models/detect/gelan-c.yaml \\\n",
    "     --hyp hyp.scratch-high.yaml \\\n",
    "     --device cpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
