{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations\n",
    "!pip install opencv-python\n",
    "!pip install ultralytics\n",
    "!pip install roboflow\n",
    "!pip install inference supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov9'...\n",
      "remote: Enumerating objects: 325, done.\u001b[K\n",
      "remote: Total 325 (delta 0), reused 0 (delta 0), pack-reused 325\u001b[K\n",
      "Receiving objects: 100% (325/325), 2.25 MiB | 2.23 MiB/s, done.\n",
      "Resolving deltas: 100% (164/164), done.\n",
      "/home/potenza/Ortho-AUV/yolov9\n",
      "/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import load_entry_point\n"
     ]
    }
   ],
   "source": [
    "# If you don't have yolov9 set up\n",
    "!git clone https://github.com/SkalskiP/yolov9.git\n",
    "%cd yolov9\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you already have yolov9 in your repo\n",
    "%cd yolov9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in COMP400-Fish-detection-3 to yolov9:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2770/2770 [01:02<00:00, 44.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to COMP400-Fish-detection-3 in yolov9:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 7058.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key=\"HV9jB1dcJZHZd1YO5S7C\") \n",
    "project = rf.workspace(\"comp400\").project(\"comp400-fish-detection\")\n",
    "dataset = project.version(3).download(\"yolov9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train (training with on roboflow's website is easier - ignore this section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt\n",
    "!wget -P weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt\n",
    "!wget -P weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-c.pt\n",
    "!wget -P weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-e.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=weights/gelan-c.pt, cfg=models/detect/gelan-c.yaml, data=COMP400-Fish-detection-3/data.yaml, hyp=hyp.scratch-high.yaml, epochs=100, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=cpu, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=0, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "YOLOv5 ðŸš€ 1e33dbb Python-3.8.10 torch-2.2.1+cu121 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, dfl=1.5, obj_pw=1.0, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO ðŸš€ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO ðŸš€ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n",
      "  3                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      "  4                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n",
      "  5                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  6                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
      "  7                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  8                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
      "  9                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]               \n",
      " 10                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 11           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 12                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]      \n",
      " 16                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 17          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 18                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]       \n",
      " 19                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 20           [-1, 9]  1         0  models.common.Concat                    [1]                           \n",
      " 21                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
      " 22      [15, 18, 21]  1   5491411  models.yolo.DDetect                     [1, [256, 512, 512]]          \n",
      "gelan-c summary: 621 layers, 25437843 parameters, 25437827 gradients, 103.2 GFLOPs\n",
      "\n",
      "Transferred 931/937 items from weights/gelan-c.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 154 weight(decay=0.0), 161 weight(decay=0.0005), 160 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/potenza/Ortho-AUV/yolov9/COMP400-Fish-detection-3/train/la\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/potenza/Ortho-AUV/yolov9/COMP400-Fish-detection-3/valid/labe\u001b[0m\n",
      "Plotting labels to runs/train/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp2\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/3 00:00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py \\\n",
    "     --data COMP400-Fish-detection-3/data.yaml \\\n",
    "     --weights weights/gelan-c.pt \\\n",
    "     --cfg models/detect/gelan-c.yaml \\\n",
    "     --hyp hyp.scratch-high.yaml \\\n",
    "     --device cpu\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key='HV9jB1dcJZHZd1YO5S7C')\n",
    "project = rf.workspace('comp400').project('comp400-fish-detection')\n",
    "model = project.version(4).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "data_dir = \"../data4\"\n",
    "file_name = \"geo.txt\"\n",
    "\n",
    "with open(os.path.join(data_dir, file_name), \"r\") as f:\n",
    "     lines = f.readlines()\n",
    "     lines = lines[1:]  # Skip the header line\n",
    "\n",
    "filenames_raw, filenames_depth = [], []\n",
    "\n",
    "for line in lines:\n",
    "     filename_raw, filename_depth, _, _, _, _, _, _ = line.strip().split()\n",
    "     filenames_raw.append(filename_raw)\n",
    "     filenames_depth.append(filename_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 342.5, 'y': 316.0, 'width': 79.0, 'height': 42.0, 'confidence': 0.8940266370773315, 'class': 'fish1', 'class_id': 0, 'detection_id': '069dbe43-9c98-4cab-a6d0-70ce6f37994c', 'image_path': '../data1/19:52:44.171.jpg', 'prediction_type': 'ObjectDetectionModel'}\n"
     ]
    }
   ],
   "source": [
    "file = filenames_raw[0]\n",
    "prediction = model.predict(os.path.join(data_dir, file), confidence=50, overlap=50).json()[\"predictions\"][0]\n",
    "x = prediction[\"x\"]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prediction.txt\", \"w\") as f:\n",
    "     f.write(\"filename_raw filename_depth x y width height\")\n",
    "     for i in range(len(filenames_raw)):\n",
    "          prediction = model.predict(os.path.join(data_dir, filenames_raw[i]), confidence=50, overlap=50).json()[\"predictions\"]\n",
    "          for j in range(len(prediction)):\n",
    "               instance = prediction[j]\n",
    "               x = instance[\"x\"]\n",
    "               y = instance[\"y\"]\n",
    "               width = instance[\"width\"]\n",
    "               height = instance[\"height\"]\n",
    "               f.write(f\"\\n{filenames_raw[i]} {filenames_depth[i]} {x} {y} {width} {height}\")\n",
    "     \n",
    "     # uncoment next line if you want to visualize predictions\n",
    "     # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(os.path.join(data_dir, \"22:28:23.171_raw.jpg\"), confidence=50, overlap=50).save(f\"prediction_22:28:23.171_raw.jpg.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
